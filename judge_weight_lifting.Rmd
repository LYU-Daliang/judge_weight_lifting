---
title: "A Model to Tell How Well You Do Weight Lifting"
author: "LYU Daliang"
date: "September 27, 2015"
output: 
  html_document: 
    keep_md: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE)
```

# Background

Using devices such as _Jawbone Up_, _Nike FuelBand_, and _Fitbit_ it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this report, I will use data from accelerometers on the _belt_, _forearm_, _arm_, and _dumbell_ of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Based on the data, I will build a model to predict the manner in which they did the exercise. 

The data come from this source: <http://groupware.les.inf.puc-rio.br/har>.

# Build the model

```{r}
library(caret)
set.seed(1530)
# read the data
training <- read.csv('pml-training.csv', na.strings =  c(NA, ''))
# extract columns 8 to 160, discard firt 7 columns
training <- training[, 8:160]
# this function coerces `chr`, `int` and `logi` to `num`
chr2num <- function(x) {  # x - data frame
    for (colnum in 1:152) {
        if (class(x[, colnum]) == 'character' | 
            class(x[, colnum]) == 'integer' | 
            class(x[, colnum]) == 'logical') {
            x[, colnum] <- as.numeric(x[, colnum])
        }
    }
    x  # return value
}
chr2num(training) -> training
# GET RID OF ALL THE NAs! nearZeroVars() IS TOO CONSERVATIVE!
incomplete <- sapply(training, function(x) {any(is.na(x))})
training <- training[, !incomplete]
## get rid of near zero variables from training data set
# nearZeroVar(training) -> discard
# training[, -discard] -> training
# make subtraining and subtesting for examing out of sample error 
insubtraining <- createDataPartition(training$classe, p = .75, list = FALSE)
subtraining <- training[insubtraining, ]
subtesting <- training[-insubtraining, ]
```

After loading and cleaning the data, I first try to fit a model with `rpart`.

```{r, eval=FALSE, echo=TRUE}
mod_rp <- train(classe ~ ., data = subtraining, method = 'rpart', 
                preProcess = c('center', 'scale'))
mod_rp$results
```

```{r}
if (file.exists('mod_rp.RData')) {
    load('mod_rp.RData')
} else {
    # system.time({
    mod_rp <- train(classe ~ ., data = subtraining, method = 'rpart', 
                    preProcess = c('center', 'scale'))
    # })
}
mod_rp$results
```

Unfortunately, it performs poorly. So I turn to `rf`, and plug in cross validation `cv`.

```{r, eval=FALSE, echo=TRUE}
mod_rf <- train(classe ~ ., data = subtraining, method = 'rf', 
                preProcess = c('center', 'scale'), 
                trControl = trainControl(method = 'cv', number = 5))
mod_rf$results
```


```{r}
if (file.exists('mod_rf.RData')) {
    load('mod_rf.RData')
} else {
    mod_rf <- train(classe ~ ., data = subtraining, method = 'rf', 
                    preProcess = c('center', 'scale'), 
                    trControl = trainControl(method = 'cv', number = 5))
}
 mod_rf$results
```

It is ideal. For comparing purpose, let us try `gmb`.

```{r, eval=FALSE, echo=TRUE}
mod_gbm <- train(classe ~ ., data = subtraining, method = 'gbm',
                 preProcess = c('center', 'scale'))
mod_gbm$results[, -(1:3)]
```

```{r}
if (file.exists('mod_gbm.RData')) {
    load('mod_gbm.RData')
} else {
    mod_gbm <- train(classe ~ ., data = subtraining, method = 'gbm',
                     preProcess = c('center', 'scale'))
}
mod_gbm$results[, -(1:3)]
```

`gbm` model is better than `rpart`, but not as good as `rf`.

# Examining the out of sample error

```{r}
confusionMatrix(predict(mod_rf, subtesting), subtesting$classe) -> cm
```

The error rate is `r round(1 - cm$overall['Accuracy'], 4)`, quite small.

```{r}
cm$table
cm$overall
```

# Apply the algorithm to 20 test cases 

```{r}
testing <- read.csv('pml-testing.csv', na.strings =  c(NA, ''))
testing <- testing[, 8:160]
chr2num(testing) -> testing
testing <- testing[, names(testing) %in% names(subtraining)]
result <- predict(mod_rf, newdata = testing)
# mod_gbm gives identical prediction
# result <- predict(mod_gbm, newdata = testing)
result

# write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(as.vector(result))
```
